...menustart

 - [Syntax Analysis](#be60f384a98f420abf899da5f654f21f)
	 - [4.1 Introduction](#903b7a00b2099a08e9d1369fffac33fc)
		 - [4.1.1 The Role of the Parser](#db09e3b8eb4e613bf82a0fc49c46ef04)
		 - [4.1.2 Representative Grammars](#404a734496a31abcde4eec096257e383)
		 - [4.1.3 Syntax Error Handling](#0a62177a1bdeb055824f516840c228b4)
		 - [4.1.4 Error-Recovery Strategies](#e1251b5a6af9cbf01527a7b7594bc9a0)
	 - [4.2 Context-Free Grammars](#e18a685fdcbfe35bcf7b3ef29c5dbfee)
		 - [4.2.1 The Formal Definition of a Context-free Grammar](#04d3df637c892628792484e05c357c42)
		 - [4.2.2 Notational Conventions](#75f9deb3c12ebfe2f88ee567cbb334af)
		 - [4.2.3 Derivations](#d4831fa5d1357d7549aa40ad8f772c8d)
		 - [4.2.4 Parse Trees and Derivations](#b7e854d42d024d8174345cb2a65ac968)
		 - [4.2.5 Ambiguity](#059218b78640920f2df8cc7048533516)

...menuend




<h2 id="be60f384a98f420abf899da5f654f21f"></h2>
# Syntax Analysis

This chapter is devoted to parsing methods that are typically used in compilers. We first present the basic concepts , then techniques suitable for hand implemen­tation, and finally algorithms that have been used in automated tools. Since programs may contain syntactic errors, we discuss extensions of the parsing methods for recovery from common errors.

By design, every programming language has precise rules that prescribe the syntactic structure of well-formed programs. In C, for example, a program is made up of functions, a function out of declarations and statements, a statement out of expressions, and so on. 

The syntax of programming language constructs can be specified by context-free grammars or BNF (Backus-Naur Form) nota­fition, introduced in Section 2.2 . Grammars offer significant benefits for both language designers and compiler writers.

 - A grammar gives a precise, yet easy-to-understand, syntactic specification of a programming language.
 - From certain classes of grammars, we can construct automatically an efficient parser that determines the syntactic structure of a source program. 
 	- As a side benefit, the parser-construction process can reveal syntactic ambiguities and trouble spots that might have slipped through the initial design phase of a language.
 - The structure imparted to a language by a properly designed grammar is useful for translating source programs into correct object code and for detecting errors.
 - A grammar allows a language to be evolved or developed iteratively, by adding new constructs to perform new tasks. 
 	- These new constructs can be integrated more easily into an implementation that follows the gram­matical structure of the language.


---

<h2 id="903b7a00b2099a08e9d1369fffac33fc"></h2>
## 4.1 Introduction

In this section, we examine the way the parser fits into a typical compiler. We then look at typical grammars for arithmetic expressions. Grammars for ex­pressions suffice for illustrating the essence of parsing, since parsing techniques for expressions carry over to most programming constructs. This section ends with a discussion of error handling, since the parser must respond gracefully to finding that its input cannot be generated by its grammar.

---

<h2 id="db09e3b8eb4e613bf82a0fc49c46ef04"></h2>
### 4.1.1 The Role of the Parser

In our compiler model, the parser obtains a string of tokens from the lexical analyzer,  and verifies that the string of token names can be generated by the grammar for the source language. 

We expect the parser to report any syntax errors in an intelligible fashion and to recover from commonly occurring errors to continue processing the remainder of the program. 

Conceptually, for well-formed programs, the parser constructs a parse tree and passes it to the rest of the compiler for further processing. In fact, the parse tree need not be constructed explicitly, since checking and translation actions can be interspersed with parsing, as we shall see. Thus, the parser and the rest of the front end could well be implemented by a single module.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F4.1png.png)

There are three general types of parsers for grammars: 

 - universal, 
	- such as the Cocke-Younger-Kasami algorithm and Earley's algorithm
	- too inefficient to use in production compilers.
 - top-down, 
	- commonly used
	- build parse trees from the top (root) to the bottom (leaves)
	- the input to the parser is scanned from left to right, one symbol at a time.
 - and bottom-up. 
	- commonly used
	- build parse trees start from the leaves and work their way up to the root
	- the input to the parser is scanned from left to right, one symbol at a time.

The most efficient top-down and bottom-up methods work only for sub­classes of grammars, but several of these classes, particularly, LL and LR gram­mars, are expressive enough to describe most of the syntactic constructs in modern programming languages. 

> LL : Left to Right, Leftmost derivation
> LR : Left to Right, Rightmost derivation

 - Parsers implemented by hand often use LL grammars; 
 	- for example, the predictive-parsing approach of Section 2.4.2 works for LL grammars.  
 - Parsers for the larger class of LR grammars are usually constructed using automated tools.

In this chapter, we assume that the output of the parser is some represent­ action of the parse tree for the stream of tokens that comes from the lexical analyzer. In practice, there are a number of tasks that might be conducted during parsing, such as 

 - collecting information about various tokens into the symbol table, 
 - performing type checking and other kinds of semantic analysis, 
 - and generating intermediate code. 

We have lumped all of these activities into the "rest of the front end" box in Fig. 4.1. These activities will be covered in detail in subsequent chapters.

---

<h2 id="404a734496a31abcde4eec096257e383"></h2>
### 4.1.2 Representative Grammars

Some of the grammars that will be examined in this chapter are presented here for ease of reference. 

Constructs that begin with keywords like **while** or **int**, are relatively easy to parse, because the keyword guides the choice of the grammar production that must be applied to match the input. We therefore concentrate on expressions, which present more of challenge, because of the associativity and precedence of operators.

Associativity and precedence are captured in the following grammar, which is similar to ones used in Chapter 2 for describing expressions, terms, and factors. *E* represents expressions consisting of terms separated by + signs, *T* represents terms consisting of factors separated by \* signs, and *F* represents factors that can be either parenthesized expressions or identifiers:

```
E → E + T | T
T → T * F | F 	(4.1) 
F → ( E ) | id
```

Expression grammar (4.1) belongs to the class of LR grammars that are suitable for bottom-up parsing. This grammar can be adapted to handle additional operators and additional levels of precedence. However, it cannot be used for top-down parsing because it is left recursive.

> left recursive 只能使用 bottom-up parsing

The following non-left-recursive variant of the expression grammar (4.1) will be used for top-down parsing:

```
E  → T E'
E' → + T E' | ε
T  → F T'			(4.2)
T' → * F T' | ε
F  → ( E ) | id  
```

The following grammar treats + and * alike, so it is useful for illustrating techniques for handling ambiguities during parsing:

```
E → E + E | E * E | ( E ) | id 		(4.3)
```

Here, E represents expressions of all types. Grammar (4.3) permits more than one parse tree for expressions like a + b \* c.


<h2 id="0a62177a1bdeb055824f516840c228b4"></h2>
### 4.1.3 Syntax Error Handling

The remainder of this section considers the nature of syntactic errors and gen­eral strategies for error recovery. 

Two of these strategies, called panic-mode and phrase-level recovery, are discussed in more detail in connection with specific parsing methods.

A compiler is expected to assist the programmer in locating and tracking down errors that inevitably creep into programs.  Planning the error handling right from the start can both simplify the structure of a compiler and improve its handling of errors.

Common programming errors can occur at many different levels.

 - *Lexical* errors include misspellings of identifiers, keywords, or operators 
 	- e.g., the use of an identifier *elipseSize* instead of *ellipseSize*
 	- and missing quotes around text intended as a string.
 - *Syntactic* errors include misplaced semicolons or extra or missing braces; 
 	- that is, "{" or "}." 
 	- As another example, in C or Java, the appearance of a case statement without an enclosing switch is a syntactic error .
 - *Semantic* errors include type mismatches between operators and operands. 
 	- An example is a return statement in a Java method with result type void.
 - *Logical* errors can be anything from incorrect reasoning on the part of the programmer 
 	- to the use in a C program of the assignment operator = instead of the comparison operator ==. 

The precision of parsing methods allows syntactic errors to be detected very efficiently. Several parsing methods, such as the LL and LR methods, detect an error as soon as possible; that is, when the stream of tokens from the lexical analyzer cannot be parsed further according to the grammar for the language. 

More precisely, they have the ***viable-prefix property***, meaning that they detect that an error has occurred as soon as they see a prefix of the input that cannot be completed to form a string in the language.

Another reason for emphasizing error recovery during parsing is that many errors appear syntactic, and are exposed when parsing cannot continue. A few semantic errors, such as type mismatches, can also be detected efficiently; however, accurate detection of semantic and logical errors at compile time is in general a difficult task.

The error handler in a parser has goals that are simple to state but challenging to realize:

 - Report the presence of errors clearly and accurately.
 - Recover from each error quickly enough to detect subsequent errors.
 - Add minimal overhead to the processing of correct programs.

Fortunately, common errors are simple ones, and a relatively straightforward error-handling mechanism often suffices.

How should an error handler report the presence of an error? At the very least, it must report the place in the source program where an error is detected, because there is a good chance that the actual error occurred within the previous few tokens. A common strategy is to print the offending line with a pointer to the position at which an error is detected.

---

<h2 id="e1251b5a6af9cbf01527a7b7594bc9a0"></h2>
### 4.1.4 Error-Recovery Strategies

Once an error is detected, how should the parser recover? 

The balance of this section is devoted to the following recovery strategies: panic-mode, phrase-level, error-productions, and global-correction.

**Panic-Mode Recovery**

With this method, on discovering an error, the parser discards input symbols one at a time until one of a designated set of ***synchronizing tokens*** is found. The synchronizing tokens are usually delimiters, such as semicolon or }, whose role in the source program is clear and unambiguous. 

The compiler designer must select the synchronizing tokens appropriate for the source language. While panic-mode correction often skips a considerable amount of input without check­ing it for additional errors, it has the advantage of simplicity, and, unlike some methods to be considered later, is guaranteed not to go into an infinite loop.


**Phrase-Level Recovery**

On discovering an error, a parser may perform local correction on the remaining input; that is, it may replace a prefix of the remaining input by some string that allows the parser to continue. A typical local correction is to replace a comma by a semicolon, delete an extraneous semicolon, or insert a missing semicolon. 

The choice of the local correction is left to the compiler designer. Of course, we must be careful to choose replacements that do not lead to infinite loops, as would be the case, for example, if we always inserted something on the input ahead of the current input symbol.

Phrase-level replacement has been used in several error-repairing compilers, as it can correct any input string. Its major drawback is the difficulty it has in coping with situations in which the actual error has occurred before the point of detection.

**Error Productions**

By anticipating common errors that might be encountered, we can augment the grammar for the language at hand with productions that generate the erroneous constructs. 

A parser constructed from a grammar augmented by these error productions detects the anticipated errors when an error production is used during parsing. The parser can then generate appropriate error diagnostics about the erroneous construct that has been recognized in the input.


**Global Correction**

Ideally, we would like a compiler to make as few changes as possible in processing an incorrect input string. 

There are algorithms for choosing a minimal sequence of changes to obtain a globally least-cost correction. 

Given an incorrect input string x and grammar G, these algorithms will find a parse tree for a related string y, such that the number of insertions, deletions, and changes of tokens required to transform x into y is as small as possible. 

Unfortunately, these methods are in general too costly to implement in terms of time and space, so these techniques are currently only of theoretical interest.

Do note that a closest correct program may not be what the programmer had in mind. Nevertheless, the notion of least-cost correction provides a yardstick for evaluating error-recovery techniques, and has been used for finding optimal replacement strings for phrase-level recovery.


---

<h2 id="e18a685fdcbfe35bcf7b3ef29c5dbfee"></h2>
## 4.2 Context-Free Grammars

---

<h2 id="04d3df637c892628792484e05c357c42"></h2>
### 4.2.1 The Formal Definition of a Context-free Grammar

A context-free grammar (grammar for short) consists of terminals, nonterminals, a start symbol, and productions.

 1. *Terminals* are the basic symbols from which strings are formed. 
 	- The term "token name" is a synonym for "terminal" and frequently we will use the word "token" for terminal when it is clear that we are talking about just the token name. 
 	- We assume that the terminals are the first components of the tokens output by the lexical analyzer. 
 	- In (4.4) , the terminals are the keywords **if** and **else** and the symbols "(" and ")".
 2. *Nonterminals* are syntactic variables that denote sets of strings. 
 	- In (4.4), *stmt* and *expr* are nonterminals. 
 	- The sets of strings denoted by nontermi­nals help define the language generated by the grammar. 
 	- Nonterminals impose a hierarchical structure on the language that is key to syntax analysis and translation.
 3. In a grammar, one nonterminal is distinguished as the *start symbol*, 
 	- and the set of strings it denotes is the language generated by the grammar. 
 	- Conventionally, the productions for the start symbol are listed first.
 4. The *productions* of a grammar specify the manner in which the termi­nals and nonterminals can be combined to form strings. Each production consists of:
 	- (a) A nonterminal called the *head* or *left side* of the production; this production defines some of the strings denoted by the head.
 	- (b) The symbol →. Sometimes ::= has been used in place of the arrow.
 	- (c) A *body or right side* consisting of zero or more terminals and non­terminals. The components of the body describe one way in which strings of the nonterminal at the head can be constructed.


Example 4.5 : The grammar in Fig. 4.2 defines simple arithmetic expressions. In this grammar, the terminal symbols are

```
	id + - * / ( )
```

The nonterminal symbols are *expression*, *term* and *factor*, and *expression* is the start symbol

```
expression → expression + term 
expression → expression - term 
expression → term
      term → term * factor 
      term → term / factor
      term → factor
    factor → ( expression )
    factor → id
```

> Figure 4.2: Grammar for simple arithmetic expressions

---

<h2 id="75f9deb3c12ebfe2f88ee567cbb334af"></h2>
### 4.2.2 Notational Conventions

To avoid always having to state that "these are the terminals," "these are the nonterminals," and so on, the following notational conventions for grammars will be used throughout the remainder of this book.

 1. These symbols are terminals:
 	- Lowercase letters early in the alphabet, such as *a*, *b*, *c*.
 	- Operator symbols such as +, *, and so on.
 	- Punctuation symbols such as parentheses, comma, and so on.
 	- The digits 0,1, ... ,9.
 	- Boldface strings such as **id** or **if**, each of which represents a single terminal symbol.
 2. These symbols are nonterminals:
 	- Uppercase letters early in the alphabet, such as **A**, **B**, **C**.
 	- The letter ***S***, which, when it appears, is usually the start symbol.
 	- Lowercase, italic names such as *expr* or *stmt*.
 	- When discussing programming constructs, uppercase letters may be used to represent nonterminals for the constructs. For example, non­ terminals for expressions, terms, and factors are often represented by E, T, and F, respectively.
 3. Uppercase letters late in the alphabet, such as X, Y, Z, represent *grammar symbols*; that is, either nonterminals or terminals.
 	- 注意，X,Y,Z 的特别之处
 4. Lowercase letters late in the alphabet, chie y u, v, . . . , z, represent (pos­ sibly empty) ***strings*** of terminals.
 5. Lowercase Greek letters, α,β,γ for example, represent (possibly empty) ***strings*** of grammar symbols. 
 	- Thus, a generic production can be written as A → α  , where A is the head and α the body.
 6. A set of productions A → α₁ , A → α₂ , ... ,A → α<sub>k</sub>  with a common head A (call them ***A-productions***), may be written A → α₁|α₂|...|α<sub>k</sub>. Call α₁, α₂, ... , α<sub>k</sub> the *alternatives* for A.
 7. Unless stated otherwise, the head of the first production is the *start* sym­ bol.

Example 4.6 : Using these conventions, the grammar of Example 4.5 can be rewritten concisely as

```
E → E + T | E - T | T 
T → T * F | T / F | F 
F → ( E ) | id
```

The notational conventions tell us that E, T, and F are nonterminals, with E the start symbol. The remaining symbols are terminals.

---

<h2 id="d4831fa5d1357d7549aa40ad8f772c8d"></h2>
### 4.2.3 Derivations

The construction of a parse tree can be made precise by taking a derivational view, in which productions are treated as rewriting rules. 

Beginning with the start symbol, each rewriting step replaces a nonterminal by the body of one of its productions. This derivational view corresponds to the ***top-down construction*** of a parse tree, but the precision a orded by derivations will be especially helpful when bottom-up parsing is discussed. As we shall see, *bottom-up parsing* is related to a class of derivations known as "rightmost" derivations, in which the rightmost nonterminal is rewritten at each step.

For example, consider the following grammar, with a single nonterminal E, which adds a production E → - E to the grammar (4.3):

```
E → E + E | E * E | - E | ( E ) | id    (4.7) 
```

The production E → - E signifies that if E denotes an expression, then - E must also denote an expression. The replacement of a single E by - E will be described by writing

```
  E ⇒ - E
```

which is read, "E derives -E." The production E → ( E ) can be applied to replace any instance of E in any string of grammar symbols by (E), e.g., E * E ⇒ (E) * E or E * E ⇒ E * (E). We can take a single E and repeatedly apply productiqns in any order to get a sequence of replacements. For example,

```
  E ⇒ - E ⇒ -(E) ⇒ -(id)
```

We call such a sequence of replacements a ***derivation*** of -(id) from E. This derivation provides a proof that the string -(**id**) is one particular instance of an expression.

For a general definition of derivation, consider a nonterminal A in the middle of a sequence of grammar symbols, as in αAβ , where α and β are arbitrary strings of grammar symbols. Suppose A → γ is a production. Then, we write αAβ ⇒ αγβ. The symbol ⇒ means, "derives in one step." When a sequence of derivation steps α₁ ⇒ α₂ ⇒ ... ⇒ αn rewrites α₁ to αn, we say α₁ ***derives*** an. 

Often, we wish to say, "derives in zero or more steps." For this purpose, we can use the symbol ⇒<sup>\*</sup>. Thus,

 1. α ⇒<sup>\*</sup> α, for any string α , and 
 2. If α ⇒<sup>\*</sup> β  and β ⇒<sup>\*</sup> γ , then α ⇒<sup>\*</sup> γ

Likewise, ⇒⁺ means, "derives in one or more steps."

If S ⇒<sup>\*</sup> α, where S is the start symbol of a grammar G, we say that α is a
*sentential form* of G. Note that a sentential form may contain both terminals and nonterminals, and may be empty. A *sentence* of G is a sentential form with no nonterminals. The *language generated by* a grammar is its set of sentences.

Thus, a string of terminals ω is in L(G), the language generated by G, if and only if ω is a sentence of G ( or S ⇒<sup>\*</sup> ω). A language that can be generated by a grammar is said to be a ***context-free language***. If two grammars generate the same language, the grammars are said to be *equivalent*.

The string -(id + id) is a sentence of grammar (4.7) because there is a derivation 

```
E ⇒ -E ⇒ -(E) ⇒ -(E+E) ⇒ -(id+E) ⇒ -(id+id)   (4.8)
```

The strings E, -E, -(E), ... , -(id + id) are all sentential forms of this grammar. We write E ⇒<sup>\*</sup> -(id + id) to indicate that -(id + id) can be derived from E.

At each step in a derivation, there are two choices to be made. We need to choose which nonterminal to replace, and having made this choice, we must pick a production with that nonterminal as head.  For example, the following alternative derivation of -(id + id) differs from derivation (4.8) in the last two steps:

```
E ⇒ -E ⇒ -(E) ⇒ -(E+E) ⇒ -(E+ id) ⇒ -(id+id)   (4.9)
```

Each nonterminal is replaced by the same body in the two derivations, but the order of replacements is different.

To understand how parsers work, we shall consider derivations in which the nonterminal to be replaced at each step is chosen as follows:

 1. In *leftmost* derivations, the leftmost nonterminal in each sentential is al­ways chosen. 
 	- If α ⇒ β is a step in which the leftmost nonterminal in α is replaced, we write 
 	- α ⇒<sub>lm</sub>β
 2. In *rightmost* derivations, the rightmost nonterminal is always chosen; 
 	- we write α ⇒<sub>rm</sub>β in this case.

Derivation (4.8) is leftmost, so it can be rewritten as

E ⇒<sub>lm</sub> -E ⇒<sub>lm</sub> -(E) ⇒<sub>lm</sub> -(E+E) ⇒<sub>lm</sub> -(id+E) ⇒<sub>lm</sub> -(id+id) 

Note that (4.9) is a rightmost derivation.

Using our notational conventions, every leftmost step can be written as ωAγ ⇒<sub>lm</sub> ωδγ, where ω consists of terminals only, A ⇒ δ is the production and γ is a string of grammar symbols. To emphasize that a derives by a leftmost derivation , we write α ⇒<sup>\*</sup><sub>lm</sub> β . If S ⇒<sup>\*</sup><sub>lm</sub> α, then we say that α is a *left-sentential form* of the grammar at hand.

Analogous definitions hold for rightmost derivations. Rightmost derivations are sometimes called *canonical* derivations.

--- 

<h2 id="b7e854d42d024d8174345cb2a65ac968"></h2>
### 4.2.4 Parse Trees and Derivations

A parse tree (CST)is a graphical representation of a derivation that filters out the order in which productions are applied to replace nonterminals. 

 - Each interior node of a parse tree represents the application of a production. 
 - The interior node is labeled with the nonterminal A in the head of the production; 
 - the children of the node are labeled, from left to right, by the symbols in the body of the production 
 	- by which this A was replaced during the derivation.

For example, the parse tree for -(id + id) in Fig. 4.3, results from the derivation (4.8) as well as derivation (4.9).

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F4.3.png)

The leaves of a parse tree are labeled by ***nonterminals or terminals*** and, read from left to right, constitute a sentential form, called the ***yield*** or ***frontier*** of the tree.

To see the relationship between derivations and parse trees, consider any derivation α₁ ⇒ α₂ ⇒ ... ⇒ αn , where `α₁` is a single nonterminal A. For each sentential form αᵢ in the derivation, we can construct a parse tree whose yield is αᵢ.  The process is an induction on i.

**BASIS**: The tree for `α₁` = A is a single node labeled A.

**INDUCTION**: Suppose we already have constructed a parse tree with yield αᵢ₋₁ = X₁X₂...X<sub>k</sub> (note that according to our notational conventions, each grammar symbol Xᵢ is either a nonterminal or a terminal). Suppose αᵢ is derived from αᵢ₋₁ by replacing Xⱼ, a nonterminal, by β = Y₁Y₂...Y<sub>m</sub>. That is, at the *i*th step of the derivation, production Xⱼ → β is applied to αᵢ₋₁ to deriveai= X₁X₂...Xⱼ₋₁βXⱼ₊₁...X.

To model this step of the derivation, find the *j*th leaf from the left in the current parse tree. This leaf is labeled Xⱼ . Give this leaf m children, labeled Y₁Y₂...Y<sub>m</sub> from the left. Asaspecialcase,if m = 0, then β = ε, and we give the *j*th leaf one child labeled ε .

Example 4.10 : The sequence of parse trees constructed from the derivation (4.8) is shown in Fig. 4.4. 

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F4.4.png)

In the first step of the derivation, E ⇒ -E. To model this step, add two children, labeled `-` and `E`, to the root `E` of the initial tree. The result is the second tree.

In the secorid step of the derivation, -E ⇒ -(E) . Consequently, add three children, labeled `(`, `E`, and `)`, to the leaf labeled `E` of the second tree, to obtain the third tree with yield -(E) . Continuing in this fashion we obtain the complete parse tree as the sixth tree.

Since a parse tree ignores variations in the order in which symbols in senten­tial forms are replaced, there is a many-to-one relationship between derivations and parse trees. For example, both derivations (4.8) and (4.9), are associated with the same final parse tree of Fig. 4.4.

In what follows, we shall frequently parse by producing a leftmost or a rightmost derivation, since there is a one-to-one relationship between parse trees and either leftmost or rightmost derivations. Both leftmost and rightmost derivations pick a particular order for replacing symbols in sentential forms, so they too  filter out variations in the order. It is not hard to show that every parse tree has associated with it a unique leftmost and a unique rightmost derivation.

---

<h2 id="059218b78640920f2df8cc7048533516"></h2>
### 4.2.5 Ambiguity

From Section 2.2.4, a grammar that produces more than one parse tree for some sentence is said to be ***ambiguous***. Put another way, an ambiguous grammar is one that produces more than one leftmost derivation or more than one rightmost derivation for the same sentence.

Example 4.11 : The arithmetic expression grammar (4.3) permits two distinct leftmost derivations for the sentence id + id * id:

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_2_distinct_leftmost_derivation.png)

The corresponding parse trees appear in Fig. 4.5.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F4.5.png)

Note that the parse tree of Fig.4.5(a) reflects the commonly assumed prece­dence of + and *, while the tree of Fig. 4.5(b) does not. 

For most parsers, it is desirable that the grammar be made unambiguous, for if it is not, we cannot uniquely determine which parse tree to select for a sentence. In other cases, it is convenient to use carefully chosen ambiguous grammars, together with ***disambiguating rules*** that "throw away" undesirable parse trees, leaving only one tree for each sentence.

---

### 4.2.6 Verifying the Language Generated by a Grammar

Although compiler designers rarely do so for a complete programming-language grammar, it is useful to be able to reason that a given set of productions gener­ates a particular language. 

Troublesome constructs can be studied by writing a concise, abstract grammar and studying the language that it generates. We shall construct such a grammar for conditional statements below.

A proof that a grammar G generates a language L has two parts: 

 - show that every string generated by G is in L, 
 - and conversely that every string in L can indeed be generated by G.

Example 4.12 : Consider the following grammar:

```
  S → (S)S | ε (4.13)
```

It may not be initially apparent, but this simple grammar generates all strings of balanced parentheses, and only such strings. To see why, we shall show first that every sentence derivable from S is balanced, and then that every balanced string is derivable from S. To show that every sentence derivable from S is balanced, we use an inductive proof on the number of steps *n* in a derivation.

**BASIS**: The basis is n = 1. The only string of terminals derivable from S in one step is the empty string, which surely is balanced.

**INDUCTION**: Now assume that all derivations of fewer than *n* steps produce balanced sentences, and consider a leftmost derivation of exactly *n* steps. Such a derivation must be of the form

&nbsp;&nbsp; S ⇒<sub>lm</sub> (S)S ⇒<sup>\*</sup><sub>lm</sub> (x)S ⇒<sup>\*</sup><sub>lm</sub> (x)y

The derivations of x and y from S take fewer than *n* steps, so by the inductive hypothesis x and y are balanced. Therefore, the string (x)y must be balanced. That is, it has an equal number of left and right parentheses, and every prefix has at least as many left parentheses as right.

Having thus shown that any string derivable from S is balanced, we must next show that every balanced string is derivable from S. To do so, use induction on the length of a string.

**BASIS**: If the string is of length 0, it must be ε, which is balanced.

**INDUCTION**: First, observe that every balanced string has even length. As­sume that every balanced string of length less than *2n* is derivable from S, and consider a balanced string *w* of length *2n*, n ≥ 1. Surely *w* begins with a left parenthesis. Let (x) be the shortest nonempty prefix of *w* having an equal number of left and right parentheses. Then *w* can be written as *w* = (x)y where  both x and y are balanced. Since x and y are of length less than 2n, they are derivable from S by the inductive hypothesis. Thus, we can find a derivation of the form

&nbsp;&nbsp; S ⇒ (S)S ⇒<sup>\*</sup> (x)S ⇒<sup>\*</sup> (x)y

proving that *w* = (x)y is also derivable from S.

---

### 4.2.7 Context-free Grammars Versus Regular Expressions

Before leaving this section on grammars and their properties, we establish that grammars are a more powerful notation than regular expressions. 

Every con­struct that can be described by a regular expression can be described by a gram­mar, but not vice-versa. Alternatively, every regular language is a context-free language, but not vice-versa.

For example, the regular expression **(a|b)\*abb** and the grammar


```
A₀ → aA₀ | bA₀ | aA₁ 
A₁ → bA₂
A₂ → bA₃
A₃ → ε
```

describe the same language, the set of strings of a's and b's ending in *abb*.

We can construct mechanically a grammar to recognize the same language as a nondeterministic finite automaton (NFA). The grammar above was con­structed from the NFA in Fig. 3.24 using the following construction:

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.24.png)

 1. For each state i of the NFA, create a nonterminal Aᵢ.
 2. If state i has a transition to state j on input a, add the production Aᵢ → aAⱼ. If state i goes to state j on input ε, add the production Aᵢ → Aⱼ.
 3. IF i is an accepting state, add Aᵢ → ε.
 4. If i is the start state, make Aᵢ be the start symbol of the grammar.

On the other hand, the language L = {aⁿbⁿ | n ≥ 1} with an equal number of a's and b's is a prototypical example of a language that can be described by a grammar but not by a regular expression. 

To see why, suppose L were the language defined by some regular expression. We could construct a DFA D with a finite number of states, say k, to accept L. Since D has only k states, for an input beginning with more than k a's, D must enter some state twice, say sᵢ, as in Fig. 4.6. 

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F4.6.png)

Suppose that the path from sᵢ back to itself is labeled with a sequence aʲ⁻¹. Since aⁱbⁱ is in the language, there must be a path labeled bⁱ from sⁱ to an accepting state f. But, then there is also a path from the initial state s₀ through sⁱ to f labeled aʲbⁱ, as shown in Fig. 4.6. Thus, D also accepts aʲbⁱ, which is not in the language, contradicting the assumption that L is the language accepted by D.

Colloquially, we say that ***"finite automata cannot count,"*** meaning that a finite automaton cannot accept a language like {aⁿbⁿ | n ≥ 1} that would require it to keep count of the number of a's before it sees the b's. Likewise, "a grammar can count two items but not three," as we shall see when we consider non-context-free language constructs in Section 4.3.5.

---

## 4.3 Writing a Grammar

Grammars are capable of describing most, but not all, of the syntax of pro­gramming languages. For instance, the requirement that identifiers be declared before they are used, cannot be described by a context-free grammar. Therefore, the sequences of tokens accepted by a parser form a superset of the program­ming language; subsequent phases of the compiler must analyze the output of the parser to ensure compliance with rules that are not checked by the parser.

This section begins with a discussion of how to divide work between a lexical analyzer and a parser. We then consider several transformations that could be applied to get a grammar more suitable for parsing. 

 - One technique can elim­inate ambiguity in the grammar, 
 - and other techniques -- left-recursion elimi­nation and left factoring -- are useful for rewriting grammars so they become suitable for top-down parsing. 

We conclude this section by considering some programming language constructs that cannot be described by any grammar.

---

### 4.3.1 Lexical Versus Syntactic Analysis

As we observed in Section 4.2.7, everything that can be described by a regular expression can also be described by a grammar. We may therefore reasonably ask: "Why use regular expressions to define the lexical syntax of a language?" There are several reasons.

 1. Separating the syntactic structure of a language into lexical and non­ lexical parts provides a convenient way of modularizing the front end of a compiler into two manageable-sized components.
 2. The lexical rules of a language are frequently quite simple, and to describe them we do not need a notation as powerful as grammars.
 3. Regular expressions generally provide a more concise and easier-to-under­ stand notation for tokens than grammars.
 4. More efficient lexical analyzers can be constructed automatically from regular expressions than from arbitrary grammars.

There are no firm guidelines as to what to put into the lexical rules, as op­posed to the syntactic rules. 

 - Regular expressions are most useful for describing the structure of constructs such as identifiers, constants, keywords, and white space. 
 - Grammars, on the other hand, are most useful for describing nested structures such as balanced parentheses, matching begin-end's, corresponding if-then-else's, and so on. 

These nested structures cannot be described by regular expressions.

---

### 4.3.2 Eliminating Ambiguity

Sometimes an ambiguous grammar can be rewritten to eliminate the ambiguity. As an example, we shall eliminate the ambiguity from the following "dangling­ else" grammar:

```
stmt → if expr then stmt 		(4.14)
	 | if expr then stmt else stmt 
	 | other
```

Here **"other"** stands for any other statement. According to this grammar, the compound conditional statement

```
if E₁ then S₁ else if E₂ then S₂ else E₃
```

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F4.8.png)

has the parse tree shown in Fig. 4.8.1 


Grammar (4.14) is ambiguous since the string

```
if E₁ then if E₂ then S₁ else S₂     (4.15)
```

has the two parse trees shown in Fig. 4.9.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F4.9.png)

In all programming languages with conditional statements of this form, the first parse tree is preferred. The general rule is, "Match each **else** with the closest unmatched **then**."  

This disambiguating rule can theoretically be in­ corporated directly into a grammar, but in practice it is rarely built into the productions.










