        <ScheduleListView
          title="Day 1"
          day={1}
          sessions={this.props.sessions}
          renderEmptyList={this.renderEmptyList}
          navigator={this.props.navigator}
        />


      <PureListView
        ref={this.storeInnerRef}
        data={this.state.todaySessions}
        renderRow={this.renderRow}
        renderSectionHeader={this.renderSectionHeader}
        {...(this.props: any /* flow can't guarantee the shape of props */)}
        renderEmptyList={this.renderEmptyList}
      />


—————— MDP ———————

T(s’ | s, a) = Pr(s’ | s, a)
	T(s,a,s’)
R(s, a)    immediate reward
	R(s,a,s’) ?   R(s, a) 这里是 期望值？
V<sup>π</sup>(s)  expected cumulative reward
     Utility = sum of (discounted) rewards
	expected total discounted rewards starting in s and following π
	V = maxQ
Q*(s,a)
	The value (utility) of a q-state (s,a):
	1.  R(s,a,s’)  +  (thereafter) acting optimally
	2. s’ 不确定， 计算期望值 ∑ T·[R + γV(s’)]


     
—————— Hierarchical Decomposition ——————
local-reward function : rewards from Is to terminal states Gi.


a hierarchical policy π 
  π = {π0, π1,...,πn}

projected value function V<sup>π</sup>(i,s)
   expected cumulative reward , from s until Mi terminates

action value function Q<sup>π</sup>(i,s,a) 


Notice that , for primitive subtasks Ma ,  V<sup>π</sup>(a,s) = R(s, a)

primitive means task tree 的叶子？

completion function C<sup>π</sup>(i,s,a)
	expected cumulative reward obtained after finishing subtask Ma , but before completing Mi


