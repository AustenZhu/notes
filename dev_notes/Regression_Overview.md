
    1. Linear regression
        1.1 hypothesis
        1.2 cost function
        1.3 Parameter learning
            1.3.1 Gradient descent
            1.3.2 Normal Equation
        1.4 practical method
            1.4.1 feature scale
            1.4.2 Mean normalization
            1.4.3 Polynomial regression
    2. Logistic Regression
        2.1 binary classification [0,1]
            2.1.1 hypothesis
            2.1.2 cost function
            2.1.3 Gradient descent
            2.1.4 decision boundary
        2.2 Advanced optimization
        2.3 Multiclass Classification: One-Vs-All
    3. Regularization
        3.1 overfitting
        3.2 regularization
            3.2.1 Linear Regression Regularization
                3.2.1.1 cost function
                3.2.1.2 gradient descent
                3.2.1.2 normal equations
            3.2.2 Logistic Regression Regularization
                3.2.2.1 cost function
                3.2.2.2 gradient descent


# Regression

---
    
## 1. Linear regression

### 1.1 hypothesis

one variable :

![](https://camo.githubusercontent.com/01623a1bba5e52e9dce1520dce62861371b77f9a/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f685f2535437468657461253238782535452537422532386925323925374425323925334425354374686574615f3026706c75733b25354374686574615f317825354525374225323869253239253744)

Multivariate matrics form:

    h(x)= θᵀX  

### 1.2 cost function

![](https://camo.githubusercontent.com/64d9116aa60e9559e21a86e91d9e2efe1c1a669c/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4a25323825354374686574615f3025324325354374686574615f312532392533442535436672616325374231253744253742326d25374425354373756d5f25374269253344312537442535452537426d2537442532386825323878253545253742253238692532392537442532392d792535452537422532386925323925374425323925354532)

### 1.3 Parameter learning

#### 1.3.1 Gradient descent

![][1]

 - LMS (least mean squares)
 - learning rate: α 
 - Gradient descent may not converge if α is set too large
 - Gradient descent may take more steps to converge if a is set too small

#### 1.3.2 Normal Equation

![](https://camo.githubusercontent.com/cd6c53e6f7b1de8f1ecfcd19045af5484cf48364/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f78253344253543626567696e253742626d6174726978253744785f25374230253744253230253543253543253230785f25374231253744253230253543253543253230785f253742322537442532302535432535432532302e2e2e253230253543253543253230785f2537426e253744253230253543656e64253742626d617472697825374425323025324325323058253344253543626567696e253742626d61747269782537442d7825354525374231542537442d2532302535432535432532302d7825354525374232542537442d2532302535432535432532302e2e2e2532302535432535432532302d782535452537426d542537442d253230253543656e64253742626d617472697825374425323025324325323079253344253543626567696e253742626d6174726978253744792535452537423125374425323025354325354325323079253545253742322537442532302535432535432532302e2e2e253230253543253543253230792535452537426d253744253230253543656e64253742626d6174726978253744)

    θ = (XᵀX)⁻¹ Xᵀy

 - if n (number of features) > 10000, Normal Equation will be very slow
 - Normal Equation can not be implemented in Logistic regression

### 1.4 practical method

#### 1.4.1 feature scale

Feature scaling will make algorithm converge more quickly.

#### 1.4.2 Mean normalization

    x = (feature-value - avg ) / range

`!! NO feature scaling on θ₀`

#### 1.4.3 Polynomial regression

Polynomial regression can fit non-linear curve, but it always cause higher training cost.

---

## 2. Logistic Regression

### 2.1 binary classification [0,1]

#### 2.1.1 hypothesis

    h(x) = g(θᵀx)

![](https://camo.githubusercontent.com/05853e89dcd20e5c82c50cdf67cdbcf18ce0cb3b/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f672532387a2532393d25354366726163253742312537442537423126706c75733b253230652535452537422d7a253744253744)

![](https://camo.githubusercontent.com/ff410969c0fb32932bc6354ea0f97e654973dc37/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f685f25354374686574612532387825323925334425354366726163253742312537442537423126706c75733b253230652535452537422d25354374686574612535452537425425374478253744253744)

`g(z):`

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/g_z.png)

`0< h(x) <1 , h(x) is the probability of y=1`
`h(x) = P( y=1|x;θ )`

#### 2.1.2 cost function

We use `log(x)` to convert h(x) to a discrete value, for computing cost function.

    -log(x) (thick) & -log(1-x): 

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/ML_logx.png)

![](https://camo.githubusercontent.com/d7d727adaf5af038cfb84ffee5a379d4a1d96c6a/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f636f7374253238253230685f25354374686574612532387825323925324379253239253344253543626567696e25374263617365732537442532302d6c6f67253238685f25354374686574612532387825323925323925323025323625323025354374657874253742253230696625323025374425323079253344253230312535432535432532302d6c6f67253238312d685f2535437468657461253238782532392532392532302532362532302535437465787425374225323069662532302537442532307925334430253230253543656e642537426361736573253744)

now we got the cost function:

    J(θ)= -1/m ∑[ ylog( h(x) ) + (1-y)log( 1-h(x) ) ]    (∑ i=1,m)

`J(θ) of logistic regression is a convex function`

#### 2.1.3 Gradient descent

same as Linear regression , but a different h(x)

![][1]

#### 2.1.4 decision boundary

`θᵀx` will descibe the decision boundary.


### 2.2 Advanced optimization

`how to:`

compute the `cost` and `partial derivatives of J(θ)` , then used in fminunc function.

 - need not pick the `learning rate - α `
 - more quickly than Gradient descent

### 2.3 Multiclass Classification: One-Vs-All

 1. use y==i to convert target value to a set with [0,1] value, and fit a classifier.
 2. we will eventually got n classifiers
 3. calculate the index of max value in n classifiers 
 
![](https://camo.githubusercontent.com/c2d77d30f9aeaebef9707762998455014d1dcfdf/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253543756e646572736574253742692537442537426d6178253744253543253243253230685f25354374686574612535452537422532386925323925374425323878253239)

---

## 3. Regularization

### 3.1 overfitting

if there is too many features, our hypothesis may fit the training sample very well, but may failed in predicting on new sample.

### 3.2 regularization

add penalty on θ to modify its weight.

#### 3.2.1 Linear Regression Regularization

##### 3.2.1.1 cost function

![](https://camo.githubusercontent.com/792f37970ec4c19ed3d09dcadd374880c3f46ff2/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4a25323825354374686574612532392533442535436672616325374231253744253742326d2537442535436c65667425323025354225323025354373756d5f25374269253344312537442535452537426d253744253238685f253543746865746125323878253545253742253238692532392537442532392d79253545253742253238692532392537442532392535453226706c75733b2535436c616d62646125323025354373756d5f2537426a253344312537442535452537426e25374425354374686574615f2537426a253744253545253742322537442532302535437269676874253230253544)

 - regularization part has no θ₀
 - if λ is too large, h(x) ≈ θ₀ , underfitting
 - if λ is too small, overfitting

##### 3.2.1.2 gradient descent

![](https://camo.githubusercontent.com/399d771f19e918f975b2d285705b7553a3134f40/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354374686574615f6a25323025334125334425323025354374686574615f6a2532302d253230253543616c7068612535436c65667425323025354225354366726163253742312537442537426d25374425354373756d5f25374269253344312537442535456d253238685f253543746865746125323878253545253742253238692532392537442532392d792535452537422532386925323925374425323978253545253742253238692532392537445f6a25323026706c75733b253543667261632537422535436c616d6264612532302537442537426d25374425354374686574615f6a2532302535437269676874253230253544)

=>

![][2]

 - should handle in 2 cases,  θ₀ has no regularization

##### 3.2.1.2 normal equations

![](https://camo.githubusercontent.com/593ce2c63b9577c592e5cad5345c0f4cafa55564/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354374686574612533442535436c65667425323025323825323058253545545826706c75733b2535436c616d626461253543626567696e253742626d617472697825374425323030253230253236253230253236253230253236253230253543253543253230253236253230312532302532362532302532362532302535432535432532302532362532302532362532302e2e2e25323025323625323025354325354325323025323625323025323625323025323625323031253230253543656e64253742626d617472697825374425323025354372696768742532302532392535452537422d31253744582535455479)



#### 3.2.2 Logistic Regression Regularization

##### 3.2.2.1 cost function

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/logistical_regression_cost.png)

##### 3.2.2.2 gradient descent

same as Linear Regression Regularization , but a different h(x)

![][2]

 - should handle in 2 cases,  θ₀ has no regularization



 ---

 [1]: https://camo.githubusercontent.com/f37f90037534d71416e113f8efab0ee2e5df2a03/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f253543746865746125334125334425354374686574612d253543616c70686125354366726163253742312537442537426d25374425354373756d5f25374269253344312537442535452537426d253744253542253238685f253543746865746125323878253545253742253238692532392537442532392d792535452537422532386925323925374425323925354363646f742532307825354525374225323869253239253744253230253544
 [2]: https://camo.githubusercontent.com/4c0bd41fad26a6629a7d3667c9afbdd22ce2c484/687474703a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f25354374686574615f6a25323025334125334425323025354374686574615f6a253238312d253543616c706861253543667261632537422535436c616d6264612537442537426d2537442532392532302d253230253543616c70686125354366726163253742312537442537426d25374425354373756d5f25374269253344312537442535456d253238685f253543746865746125323878253545253742253238692532392537442532392d792535452537422532386925323925374425323978253545253742253238692532392537445f6a
