


# CS294: Deep Reinforcement Learning, Spring 2017

http://rll.berkeley.edu/deeprlcourse

https://www.reddit.com/r/berkeleydeeprlcourse/


# Week1 : Introduction

## Why deep reinforcement learning?

 - Deep = can process complex sensory input
    - … and also compute really complex functions
 - Reinforcement learning = can choose complex actions

## What is Reinforcement Learning?

 - agent interacting with a previously unknown environment, trying to maximize cumulative reward
 - Formalized as partially observable Markov decision process (POMDP)


## Motor Control and Robotics

Robotics:

 - Observations: camera images, joint angles
 - Actions: joint torques
 - Rewards: stay balanced, navigate to target locations, serve and protect humans


## Business Operations

Inventory Management

 - Observations: current inventory levels
 - Actions: number of units of each item to purchase
 - Rewards: profit


## What Is Deep Reinforcement Learning?

 - Reinforcement learning using neural networks to approximate functions
    - Policies (select next action)
    - Value functions ( **measure goodness of states or state-action pairs** )
    - Dynamics Models (predict next states and rewards)
        - try to approximate how the system is going to evolve over time

## How Does RL Relate to Other ML Problems?

 - Reinforcement learning:
    - Environment samples input **x<sub>t<sub> ~ P(x<sub>t<sub> | x<sub>t-1<sub>, y<sub>t-1<sub>)**
        - **Environment is stateful: input depends on your previous actions!**
        - Agent takes action ŷt = f(x<sub>t<sub>)
    - Agent receives cost c<sub>t<sub> ~ P(c<sub>t<sub> | x<sub>t<sub> , ŷ<sub>t<sub> ) where P a probability distribution unknown to the agent.


# Week2 : Supervised learning and decision making (Levine)

## Terminology & notation 

 - x<sub>t<sub> : state
 - o<sub>t<sub> : observation
 - u<sub>t<sub> : action
 - π(u<sub>θ<sub>|o<sub>t<sub>)  : policy
 
 - c : cost function
 - r : reward 
    - c = - r



